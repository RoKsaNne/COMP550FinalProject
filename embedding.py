# -*- coding: utf-8 -*-
"""embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZEqWBUqCH7VYWl2VvuSrgcFtrfzMSE7P
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

# Commented out IPython magic to ensure Python compatibility.
def load_dataset(train_lang, test_lang):
    from google.colab import drive
    drive.mount('/content/drive')
#     %cd /content/drive/My Drive/colab/new

    path = 'train.tsv'
    path2 = f"translated_{test_lang}.tsv"
    # on parallel corpus
    data1 = pd.read_csv(path, sep='\t', nrows=10000,error_bad_lines=False)
    data2 = pd.read_csv(path2, sep='\t', nrows=10000,error_bad_lines=False)
    df1_sentences = data1['sentence1'] + " " + data1['sentence2']
    df1_labels = data1['label'].tolist()
    df2_sentences = data2['sentence1'] + " " + data2['sentence2']
    df2_labels = data2['label'].tolist()
    df1_sentences = [str(text) for text in df1_sentences]
    df2_sentences = [str(text) for text in df2_sentences]
    # remove Nan
    df1_sentences_clean = [text for text, label in zip(df1_sentences, df1_labels) if not pd.isna(label)]
    df1_labels_clean = [label for label in df1_labels if not pd.isna(label)]
    df2_sentences_clean = [text for text, label in zip(df2_sentences, df2_labels) if not pd.isna(label)]
    df2_labels_clean = [label for label in df2_labels if not pd.isna(label)]

    return df1_sentences_clean, df1_labels_clean, df2_sentences_clean, df2_labels_clean
def load_new_dataset(lang):
    from google.colab import drive
    drive.mount('/content/drive')
#     %cd /content/drive/My Drive/colab/new
    if lang == 'en':
        path = 'train.tsv'
    else:
        path = f"translated_{lang}.tsv"
    # on brand new corpus
    data = pd.read_csv(path, sep='\t', skiprows=range(1, 10001),nrows=10000,error_bad_lines=False)
    df_sentences = data['sentence1'] + " " + data['sentence2']
    df_labels = data['label'].tolist()
    df_sentences = [str(text) for text in df_sentences]
    # remove Nan
    df_sentences_clean = [text for text, label in zip(df_sentences, df_labels) if not pd.isna(label)]
    df_labels_clean = [label for label in df_labels if not pd.isna(label)]

    return df2_sentences_clean, df2_labels_clean

!pip install transformers
from transformers import BertTokenizer, BertModel
import torch
from transformers import DistilBertTokenizer, DistilBertModel

def create_bert_embeddings(sentences, tokenizer, model):
    embeddings = []
    for sent in sentences:
        encoded_dict = tokenizer.encode_plus(
                            sent,
                            add_special_tokens = True,
                            max_length = 64,
                            pad_to_max_length = True,
                            return_attention_mask = True,
                            return_tensors = 'pt',
                        )

        with torch.no_grad():
            outputs = model(**encoded_dict)
            cls_embedding = outputs[0][:, 0, :].numpy()
            embeddings.append(cls_embedding)

    embeddings = np.vstack(embeddings)
    return embeddings
def load_translate_embed_dataset(df1_sentences,df2_sentences,df1_labels,df2_labels):

    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')
    model = DistilBertModel.from_pretrained('distilbert-base-cased')
    df1_embeddings = create_bert_embeddings(df1_sentences, tokenizer, model)
    df2_embeddings = create_bert_embeddings(df2_sentences, tokenizer, model)
    return df1_embeddings, df1_labels, df2_embeddings, df2_labels

# set lang !!replace here!!
train_lang = 'en'
test_lang = 'zh'
df1_sentences, df1_labels, df2_sentences, df2_labels = load_dataset(test_lang)
df1_embeddings, df1_labels, df2_embeddings, df2_labels = load_translate_embed_dataset(df1_sentences,df2_sentences,df1_labels,df2_labels)

def train_and_evaluate_classifier(clf,X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_test)
    r=classification_report(y_test, predictions)
    print(r)
    return r


df3_sentences_clean, df3_labels_clean = load_new_dataset(test_lang)
# df3_sentences_clean, df3_labels_clean = load_new_dataset('en')

df3_embeddings = create_bert_embeddings(df3_sentences_clean, tokenizer, model)


svm_classifier = SVC()
rf_classifier = RandomForestClassifier()
knn_classifier = KNeighborsClassifier(n_neighbors=2)
logistic_classifier = LogisticRegression()

# embed 
print("on parallel corpus")
svm_report_tf=train_and_evaluate_classifier(svm_classifier,df1_embeddings, df1_labels, df2_embeddings, df2_labels)
knn_report_tf=train_and_evaluate_classifier(knn_classifier,df1_embeddings, df1_labels, df2_embeddings, df2_labels)
lr_report_tf=train_and_evaluate_classifier(logistic_classifier,df1_embeddings, df1_labels, df2_embeddings, df2_labels)
print("with brand new testing dataset")
svm_classifier2 = SVC()
knn_classifier2 = KNeighborsClassifier(n_neighbors=2)
logistic_classifier2 = LogisticRegression()
svm_report_tf=train_and_evaluate_classifier(svm_classifier2,df1_embeddings, df1_labels, df3_embeddings, df3_labels_clean)
knn_report_tf=train_and_evaluate_classifier(knn_classifier2,df1_embeddings, df1_labels, df3_embeddings, df3_labels_clean)
lr_report_tf=train_and_evaluate_classifier(logistic_classifier2,df1_embeddings, df1_labels, df3_embeddings, df3_labels_clean)